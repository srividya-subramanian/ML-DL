# -*- coding: utf-8 -*-
"""
Created on Mon Mar 31 20:05:45 2025

@author: srivi
"""

There are several other techniques that help handle class imbalance in datasets. 
These techniques can be broadly categorized into oversampling, undersampling, and 
hybrid approaches.

1ï¸âƒ£ Oversampling Techniques (Like SMOTE)
Oversampling increases the number of minority class samples to balance the dataset.

ğŸ”¹ Variants of SMOTE
ğŸ”¹ Borderline-SMOTE: Focuses on generating synthetic data near the decision boundary.
ğŸ”¹ ADASYN (Adaptive Synthetic Sampling): Creates more synthetic samples for 
harder-to-classify minority points.
ğŸ”¹ SMOTE-NC (SMOTE for Nominal and Continuous features): Handles datasets with 
both categorical and numerical variables.
ğŸ”¹ K-Means SMOTE: Uses K-Means clustering to ensure new synthetic samples are more representative.

GAN-Based Data Augmentation (Generative Adversarial Networks)

Uses deep learning to generate realistic synthetic samples for the minority class.
Suitable for complex fraud detection or medical diagnosis applications.


2ï¸âƒ£ Undersampling Techniques
Undersampling removes excess majority class samples to balance the dataset.

ğŸ”¹ Random Undersampling (RUS)
Randomly removes majority class samples.
Risk: May discard valuable information.
ğŸ”¹ Tomek Links
Identifies majority class samples that are very close to minority class samples and removes them.
Improves class separation.
ğŸ”¹ Edited Nearest Neighbors (ENN)
Removes majority samples that are misclassified by their k-nearest neighbors.
Reduces overlapping between classes.
ğŸ”¹ NearMiss
Selects only the most difficult majority class samples for training.
Ensures that the model learns from hard cases.

3ï¸âƒ£ Hybrid Techniques (Combining Oversampling & Undersampling)
These methods combine oversampling and undersampling to get the best of both worlds.

ğŸ”¹ SMOTE + Tomek Links
SMOTE creates synthetic minority samples.
Tomek Links removes redundant majority samples.

ğŸ”¹ SMOTE + Edited Nearest Neighbors (SMOTE + ENN)
ENN removes noisy samples after SMOTE oversampling.
Results in a cleaner, better-separated dataset.

ğŸ”¹ BalancedBaggingClassifier
Uses bootstrapping and undersampling within an ensemble model.
Helps in fraud detection and real-world class imbalance scenarios.

Choosing the Right Technique
Technique	  Pros	                                        Cons	                    Best Use Cases
SMOTE	      Prevents overfitting, generates new data	    Can create noise	        Fraud detection, medical diagnosis
ADASYN	      Focuses on hard-to-learn samples	            Can create more overlap	    Fraud, rare disease prediction
Tomek Links	  Reduces noise and improves decision boundary	May lose useful data	    Credit card fraud, spam detection
NearMiss	  Keeps only hard-to-classify samples	        May remove useful data	    Text classification, sentiment analysis
SMOTE + ENN	  Balances and cleans dataset	                Computationally expensive	Image classification, predictive maintenance

Conclusion
ğŸ”¹ For large datasets â†’ Use SMOTE or ADASYN.
ğŸ”¹ For noise-prone datasets â†’ Use Tomek Links or ENN.
ğŸ”¹ For small datasets â†’ Consider GANs or Hybrid approaches.
ğŸ”¹ For deep learning applications â†’ Try GAN-based augmentatio




























