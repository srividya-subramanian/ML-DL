# -*- coding: utf-8 -*-
"""
Created on Thu Feb 27 09:40:01 2025

@author: srivi
"""

 
How RNN (Recurrent Neural Network) Works
RNNs are a type of neural network designed for sequential data, such as text, 
speech, and time series. Unlike traditional feedforward networks, RNNs maintain 
a memory of previous inputs through hidden states.

1ï¸âƒ£ The Core Idea of RNN
RNNs process data sequentially and pass information from previous steps to 
influence the current step.

Each time step receives an input ğ‘¥ğ‘¡ and a hidden state ht from the previous step.
The hidden state acts as a memory of past information.
The network updates its hidden state using a weight matrix and a non-linear activation function.

Mathematical Formula:
ht =tanh(Wx * xt +Wh* h(tâˆ’1)+b)
yt =Wy* ht +byâ€‹
 
Where:

ht  is the hidden state at time step ğ‘¡
xt  is the input at time ğ‘¡
ğ‘Šğ‘¥,ğ‘Šâ„,ğ‘Šğ‘¦  are weight matrices.
ğ‘,ğ‘ğ‘¦  are biases.â¡
tanh (or ReLU) is an activation function.

2ï¸âƒ£ Step-by-Step Process

Initialize the hidden state h0  (usually as zeros)
Process the first input x1  and update the hidden state h1
â€‹Pass h1 to the next time step along with x2
â€‹Repeat this process for all time steps
At the last time step, output yt is generated.
ğŸ“Œ Key Feature: RNNs allow previous information to influence future predictions, 
making them ideal for tasks where order matters.

3ï¸âƒ£ Example: Predicting the Next Word
Sentence: "The cat sat on the __."

RNN Flow:
x1  â†’ "The" â†’ h1
x2  â†’ "cat" â†’ h2
x3  â†’ "sat" â†’ h3
â€‹x4  â†’ "on" â†’ h4
â€‹x5  â†’ Predicts the next word â†’ "mat"

4ï¸âƒ£ Limitations of Basic RNNs
âœ… Captures short-term dependencies
âŒ Struggles with long-term dependencies (due to vanishing gradient problem)
âŒ Difficulty in remembering information from many time steps ago

ğŸš€ Solution? Use LSTMs (Long Short-Term Memory) or GRUs (Gated Recurrent Units),
 which improve memory handling.

5ï¸âƒ£ Where Are RNNs Used?
ğŸ”¹ Text Generation (e.g., Chatbots, Auto-complete)
ğŸ”¹ Speech Recognition (e.g., Siri, Google Assistant)
ğŸ”¹ Machine Translation (e.g., Google Translate)
ğŸ”¹ Time Series Forecasting (e.g., Stock Market Predictions)

ğŸ”¹ Summary
âœ… RNNs process sequential data by maintaining a memory of past inputs.
âœ… They update a hidden state at each time step to influence the next step.
âœ… While effective for short-term memory, they struggle with long-term 
dependencies, which is why LSTMs/GRUs are often preferred.