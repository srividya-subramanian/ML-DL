# -*- coding: utf-8 -*-
"""
Created on Mon Mar 31 19:46:12 2025

@author: srivi
"""

How SMOTE Works (Synthetic Minority Over-sampling Technique)
SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation method 
used to address class imbalance in datasets. It creates synthetic samples for the 
minority class instead of just duplicating existing samples.

Why Use SMOTE?
🔹 In imbalanced datasets (e.g., fraud detection, medical diagnoses), the majority 
class dominates, causing models to be biased.
🔹 If we simply duplicate minority class samples, the model may overfit and fail 
to generalize.
🔹 SMOTE generates new synthetic examples, making the dataset more balanced and 
improving model learning.

How Does SMOTE Work?

1️⃣ Select a Minority Class Sample:
Randomly pick a data point from the minority class.

2️⃣ Find k Nearest Neighbors (k-NN):
Identify k-nearest minority class neighbors based on Euclidean distance.

3️⃣ Generate Synthetic Sample:
Select a random neighbor and create a new sample along the line connecting them.

4️⃣ Repeat Until the Minority Class is Balanced:
The process continues until the desired class balance is achieved.

SMOTE Formula for New Synthetic Data Point
If 
𝑋𝑖 is the original sample and Xnn is a randomly chosen nearest neighbor, a synthetic sample is generated as:

𝑋new=𝑋𝑖+𝜆×(𝑋𝑛𝑛−𝑋𝑖)
Where: λ is a random value between 0 and 1, ensuring the new point is between 
𝑋𝑖 and X nn.

from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.model_selection import train_test_split

# Example Dataset
X = [[2, 3], [3, 5], [5, 8], [6, 9], [7, 11], [10, 15]]  # Features
y = [0, 0, 0, 1, 1, 1]  # Class labels (1 = minority)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# Apply SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before SMOTE:", Counter(y_train))
print("After SMOTE:", Counter(y_resampled))


Advantages of SMOTE
✅ Prevents Overfitting: Unlike simple oversampling, SMOTE generates new synthetic 
data rather than duplicating existing ones.
✅ Improves Model Generalization: Models trained on SMOTE-balanced datasets perform 
better on unseen data.
✅ Works Well for Small Datasets: Helps boost the impact of minority class samples.

Limitations of SMOTE
⚠ Risk of Overlapping Classes: If the minority class is too spread out, synthetic 
points may overlap with the majority class, causing misclassification.
⚠ Doesn't Work Well for Highly Imbalanced Data: If the minority class is extremely 
rare (e.g., fraud cases <1%), SMOTE may not be enough. Hybrid methods 
(SMOTE + Undersampling) work better.
⚠ Not Ideal for Categorical Data: SMOTE is designed for continuous numerical features,
making it less effective for categorical variables.


Variants of SMOTE
Borderline-SMOTE: Generates synthetic samples only near the decision boundary, 
improving classification.
ADASYN (Adaptive SMOTE): Focuses more on hard-to-classify minority samples, making 
the model even more robust.
SMOTE-Tomek Links: Combines SMOTE (oversampling) with Tomek Links (undersampling) 
to clean noise.
SMOTE-NC (SMOTE for Nominal and Continuous features): Handles datasets with both categorical and numerical variables.
K-Means SMOTE: Uses K-Means clustering to ensure new synthetic samples are more representative.

Conclusion
🔹 SMOTE is a powerful technique for handling class imbalance in fraud detection, 
medical diagnosis, and rare event prediction.
🔹 It improves model learning, but must be used carefully to avoid creating synthetic 
noise.
🔹 Combining SMOTE with undersampling techniques often gives the best results.


















