# -*- coding: utf-8 -*-
"""
Created on Mon Mar 31 19:46:12 2025

@author: srivi
"""

How SMOTE Works (Synthetic Minority Over-sampling Technique)
SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation method 
used to address class imbalance in datasets. It creates synthetic samples for the 
minority class instead of just duplicating existing samples.

Why Use SMOTE?
ğŸ”¹ In imbalanced datasets (e.g., fraud detection, medical diagnoses), the majority 
class dominates, causing models to be biased.
ğŸ”¹ If we simply duplicate minority class samples, the model may overfit and fail 
to generalize.
ğŸ”¹ SMOTE generates new synthetic examples, making the dataset more balanced and 
improving model learning.

How Does SMOTE Work?

1ï¸âƒ£ Select a Minority Class Sample:
Randomly pick a data point from the minority class.

2ï¸âƒ£ Find k Nearest Neighbors (k-NN):
Identify k-nearest minority class neighbors based on Euclidean distance.

3ï¸âƒ£ Generate Synthetic Sample:
Select a random neighbor and create a new sample along the line connecting them.

4ï¸âƒ£ Repeat Until the Minority Class is Balanced:
The process continues until the desired class balance is achieved.

SMOTE Formula for New Synthetic Data Point
If 
ğ‘‹ğ‘– is the original sample and Xnn is a randomly chosen nearest neighbor, a synthetic sample is generated as:

ğ‘‹new=ğ‘‹ğ‘–+ğœ†Ã—(ğ‘‹ğ‘›ğ‘›âˆ’ğ‘‹ğ‘–)
Where: Î» is a random value between 0 and 1, ensuring the new point is between 
ğ‘‹ğ‘– and X nn.

from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.model_selection import train_test_split

# Example Dataset
X = [[2, 3], [3, 5], [5, 8], [6, 9], [7, 11], [10, 15]]  # Features
y = [0, 0, 0, 1, 1, 1]  # Class labels (1 = minority)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# Apply SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before SMOTE:", Counter(y_train))
print("After SMOTE:", Counter(y_resampled))


Advantages of SMOTE
âœ… Prevents Overfitting: Unlike simple oversampling, SMOTE generates new synthetic 
data rather than duplicating existing ones.
âœ… Improves Model Generalization: Models trained on SMOTE-balanced datasets perform 
better on unseen data.
âœ… Works Well for Small Datasets: Helps boost the impact of minority class samples.

Limitations of SMOTE
âš  Risk of Overlapping Classes: If the minority class is too spread out, synthetic 
points may overlap with the majority class, causing misclassification.
âš  Doesn't Work Well for Highly Imbalanced Data: If the minority class is extremely 
rare (e.g., fraud cases <1%), SMOTE may not be enough. Hybrid methods 
(SMOTE + Undersampling) work better.
âš  Not Ideal for Categorical Data: SMOTE is designed for continuous numerical features,
making it less effective for categorical variables.


Variants of SMOTE
Borderline-SMOTE: Generates synthetic samples only near the decision boundary, 
improving classification.
ADASYN (Adaptive SMOTE): Focuses more on hard-to-classify minority samples, making 
the model even more robust.
SMOTE-Tomek Links: Combines SMOTE (oversampling) with Tomek Links (undersampling) 
to clean noise.
SMOTE-NC (SMOTE for Nominal and Continuous features): Handles datasets with both categorical and numerical variables.
K-Means SMOTE: Uses K-Means clustering to ensure new synthetic samples are more representative.

Conclusion
ğŸ”¹ SMOTE is a powerful technique for handling class imbalance in fraud detection, 
medical diagnosis, and rare event prediction.
ğŸ”¹ It improves model learning, but must be used carefully to avoid creating synthetic 
noise.
ğŸ”¹ Combining SMOTE with undersampling techniques often gives the best results.


















