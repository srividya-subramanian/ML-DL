# -*- coding: utf-8 -*-
"""
Created on Wed Feb 12 09:30:21 2025

@author: srivi
"""
'''
Gaussian mixture model -  a probablistic model
- assumes all data points are generated from a mixture of finite number of 
  Gaussian distributions with unknown parameters.
- enables one to learn Gaussian Mixture Models (diagonal, spherical, tied and 
  full covariance matrices supported), sample them, and estimate them from data
Pros: speed and it is agnostic - meaning: maximizes likelyhood, but will not 
      bias the means to zero
Cons: Singularities - data with insufficient no. of points makes the estimation
      of covariance difficult and algorithm will find infinite likelyhood unless
      covariances are regularized artificially
      No. of Components:  use all the components, with out theoretical criteria
      to decide how many components are required to be used
How to train a generative model
• Assume an underlying model that lead to generation of the dataset
• The model is generally a mixture of components (e.g. Gaussians)
• The model parameters are learned such that the dataset has maximum likelihood
of being generated
Probability Mixture Models
• Probabilistic version of clustering
• Dataset is modelled as mixture of Gaussians
• Inverse of probability density can be used as anomaly score 
Gaussian Mixture Model (GMM)
 • A probabilistic generative model
 • Assumes the data is generated by a mixture of Gaussian distributions
 • The mixture components can represent normal data only or 
 both normal data and anomalies     


The Gaussian Distribution
• Univariate density
�(x |μ, σ) = 1/sqrt(2 * πσ**2)e**−((x − μ)**2/2σ**2)
• Multivariate density
�(x|μ, Σ) =1/sqrt(2 * π|Σ|)e**−((((x − μ)**T)*Σ**-1(x − μ))/2)


'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy.random as rand
import sklearn 

from sklearn.datasets import load_digits
digits = load_digits(n_class=10)
print(digits.data.shape)

import matplotlib.pyplot as plt
plt.gray()
plt.matshow(digits.images[0])
plt.show()


data = digits.data
images = digits.images
labels = digits.target

label0 = labels[np.where(labels == 0)]
data0 = data[np.where(labels == 0)]

label1 = labels[np.where(labels == 1)]
data1 = data[np.where(labels == 1)]

label2 = labels[np.where(labels == 2)]
data2 = data[np.where(labels == 2)]

label3 = labels[np.where(labels == 3)]
data3 = data[np.where(labels == 3)]

label4 = labels[np.where(labels == 4)]
data4 = data[np.where(labels == 4)]

label5 = labels[np.where(labels == 5)]
data5 = data[np.where(labels == 5)]

label6 = labels[np.where(labels == 6)]
data6 = data[np.where(labels == 6)]

label7 = labels[np.where(labels == 7)]
data7 = data[np.where(labels == 7)]

label8 = labels[np.where(labels == 8)]
data8 = data[np.where(labels == 8)]

label9 = labels[np.where(labels == 9)]
data9 = data[np.where(labels == 9)]

label = [label0,label1,label2,label3,label4,label5,label6,label7,label8,label9]
data = [data0, data1,data2, data3,data4, data5,data6, data7,data8, data9]

data_new = []
label_new = []

for i in range(0,10):
    print(i)
    Y = label[i]
    X = data[i]

    from sklearn.mixture import GaussianMixture as GMM
    model = GMM(n_components = 1, n_init = 5, init_params='random', 
            covariance_type='diag', max_iter = 100,random_state=42)#, init_params='k-means++'

    model.fit(X)

    new = model.sample(10)
    print(new[0].shape)
    X_new = new[0]
    y_new = new[1]
   
    data_new.append(X_new)
    label_new.append(y_new)
    
    plt.imshow(X_new[0].reshape(8, 8), cmap='binary')
    plt.show()
    print(y_new[0])


y_pred = model.predict(X)
y=Y

#for dist in range(10):
#    part = Y[np.where(preds==dist)]
#    print(part)


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y, y_pred)
#precision = precision_score(y, y_pred)#, pos_label='0')
#recall = recall_score(y, y_pred)#, pos_label='0')
#f1score = f1_score(y, y_pred)#, pos_label='0')

print(accuracy)#, precision, recall, f1score)

def plot_digits(data):
    fig, ax = plt.subplots(10, 10, figsize=(8, 8),
                           subplot_kw=dict(xticks=[], yticks=[]))
    fig.subplots_adjust(hspace=0.05, wspace=0.05)
    for i, axi in enumerate(ax.flat):
        im = axi.imshow(data[i].reshape(8, 8), cmap='binary')
        im.set_clim(0, 16)
plot_digits(digits.data)





