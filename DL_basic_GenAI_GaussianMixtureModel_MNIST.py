# -*- coding: utf-8 -*-
"""
Created on Wed Feb 12 09:30:21 2025

@author: srivi
"""
'''
Gaussian mixture model -  a probablistic model
- assumes all data points are generated from a mixture of finite number of 
  Gaussian distributions with unknown parameters.
- enables one to learn Gaussian Mixture Models (diagonal, spherical, tied and 
  full covariance matrices supported), sample them, and estimate them from data
Pros: speed and it is agnostic - meaning: maximizes likelyhood, but will not 
      bias the means to zero
Cons: Singularities - data with insufficient no. of points makes the estimation
      of covariance difficult and algorithm will find infinite likelyhood unless
      covariances are regularized artificially
      No. of Components:  use all the components, with out theoretical criteria
      to decide how many components are required to be used
How to train a generative model
• Assume an underlying model that lead to generation of the dataset
• The model is generally a mixture of components (e.g. Gaussians)
• The model parameters are learned such that the dataset has maximum likelihood
of being generated
Probability Mixture Models
• Probabilistic version of clustering
• Dataset is modelled as mixture of Gaussians
• Inverse of probability density can be used as anomaly score 
Gaussian Mixture Model (GMM)
 • A probabilistic generative model
 • Assumes the data is generated by a mixture of Gaussian distributions
 • The mixture components can represent normal data only or 
 both normal data and anomalies     


The Gaussian Distribution
• Univariate density
�(x |μ, σ) = 1/sqrt(2 * πσ**2)e**−((x − μ)**2/2σ**2)
• Multivariate density
�(x|μ, Σ) =1/sqrt(2 * π|Σ|)e**−((((x − μ)**T)*Σ**-1(x − μ))/2)
'''

# Project goal: to generate new data for each digits from MNIST data



# load basic libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy.random as rand
import sklearn 

# load MNIST data from sklearn
from sklearn.datasets import load_digits
digits = load_digits(n_class=10)
print(digits.data.shape)

# visualise a sample image
import matplotlib.pyplot as plt
plt.gray()
plt.matshow(digits.images[0])
plt.show()

# data and label corresponding to each digit
data = digits.data
images = digits.images
labels = digits.target

label0 = labels[np.where(labels == 0)]
data0 = data[np.where(labels == 0)]

label1 = labels[np.where(labels == 1)]
data1 = data[np.where(labels == 1)]

label2 = labels[np.where(labels == 2)]
data2 = data[np.where(labels == 2)]

label3 = labels[np.where(labels == 3)]
data3 = data[np.where(labels == 3)]

label4 = labels[np.where(labels == 4)]
data4 = data[np.where(labels == 4)]

label5 = labels[np.where(labels == 5)]
data5 = data[np.where(labels == 5)]

label6 = labels[np.where(labels == 6)]
data6 = data[np.where(labels == 6)]

label7 = labels[np.where(labels == 7)]
data7 = data[np.where(labels == 7)]

label8 = labels[np.where(labels == 8)]
data8 = data[np.where(labels == 8)]

label9 = labels[np.where(labels == 9)]
data9 = data[np.where(labels == 9)]

label = [label0,label1,label2,label3,label4,label5,label6,label7,label8,label9]
data = [data0, data1,data2, data3,data4, data5,data6, data7,data8, data9]

# generate a sample of 10 data for each digit and save them

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.mixture import GaussianMixture as GMM

for i in range(0,10):

    Y = label[i]
    X = data[i]

    model = GMM(n_components = 1, n_init = 5, init_params='random', 
            covariance_type='diag', max_iter = 100,random_state=42)#, init_params='k-means++'
    model.fit(X)

    new = model.sample(10)
    print(new[0].shape)
    X_new = new[0]
    y_new = new[1]
    if i == 0:
        data_new = X_new
        label_new = y_new
    else:
        data_new = np.concatenate((data_new, X_new),axis=0)
        label_new =  np.concatenate((label_new, y_new),axis=0)
    

'''#    y_pred = model.predict(X)
#    y=Y
#    plt.imshow(X_new[0].reshape(8, 8), cmap='binary')
#    plt.show()
#    print(y_new[0])
# accuracy = accuracy_score(y, y_pred)
# precision = precision_score(y, y_pred)#, pos_label='0')
# recall = recall_score(y, y_pred)#, pos_label='0')
# f1score = f1_score(y, y_pred)#, pos_label='0')
# print(accuracy)#, precision, recall, f1score)'''




# visualise the generated data
# this works as well: 

'''
fig, ax = plt.subplots(10,10,figsize=(10,10))
for i in range(10):
    for k in range(10):
        ax[i][k].imshow(data_new[i+k*10].reshape(8,8),cmap='gray')
        ax[i][k].axis('off')
plt.show()
'''

# visualise the generated data
# another way of plotting
def plot_digits(data):
    fig, ax = plt.subplots(10, 10, figsize=(8, 8),
                           subplot_kw=dict(xticks=[], yticks=[]))
    fig.subplots_adjust(hspace=0.05, wspace=0.05)
    
    for i in range(10):
        for j in range(10):
            im = ax[i][j].imshow(data_new[i+j*10].reshape(8,8), cmap='binary')
plt.show()
plot_digits(data_new)


