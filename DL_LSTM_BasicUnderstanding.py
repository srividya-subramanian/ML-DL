# -*- coding: utf-8 -*-
"""
Created on Tue Feb 25 04:59:00 2025

@author: srivi
ðŸ”¹ Long Short-Term Memory (LSTM) Networks
LSTM (Long Short-Term Memory) is a special type of Recurrent Neural Network (RNN) 
designed to handle long-term dependencies and sequential data more effectively 
than standard RNNs. It overcomes the vanishing gradient problem, allowing networks
to remember information over longer sequences.

ðŸ”¹ Why Use LSTMs?
ðŸš€ Captures long-range dependencies â€“ Retains memory of past inputs for a long time
ðŸš€ Prevents vanishing gradient problem â€“ Efficiently trains deep networks
ðŸš€ Ideal for sequential tasks â€“ Used in NLP, speech recognition, time series forecasting

ðŸ”¹ LSTM vs. Standard RNN
Feature	Standard            RNN	                    LSTM
Memory Retention	       Short-term only	    Long-term (with memory cells)
Vanishing Gradient	     Severe issue	        Mitigated
Suitable for	       Short sequences	        Long sequences
Example Use Case	Simple text generation	    Sentiment analysis, translation

ðŸ”¹ LSTM Architecture
Unlike a simple RNN that only has a single activation function, an LSTM unit has three gates:

Forget Gate â€“ Decides what information to discard from memory.
Input Gate â€“ Determines which new information to store.
Output Gate â€“ Decides the final output of the cell.

ðŸ”· Cell State (C_t) stores long-term memory, modified by the gates.
ðŸ”· Hidden State (h_t) carries information to the next timestep.

ðŸ”¹ LSTM Implementation Without TensorFlow/Keras (NumPy Only)
We implement a simple LSTM forward pass using NumPy.

ðŸ”¹ Applications of LSTMs
âœ… Speech Recognition â€“ Converts speech into text
âœ… Machine Translation â€“ Translates one language to another
âœ… Stock Market Prediction â€“ Forecasts financial trends
âœ… Chatbots â€“ Used in conversational AI
"""

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))



vocab_size=26
embedding_dim=128
hidden_size=256
output_size=26
vocab_size = vocab_size
embedding_dim = embedding_dim
hidden_size = hidden_size
output_size = output_size

# Embedding layer
embedding = np.random.randn(vocab_size, embedding_dim) * 0.01

# LSTM Weights
W_i = np.random.randn(hidden_size, embedding_dim) * 0.01  # Input gate
W_f = np.random.randn(hidden_size, embedding_dim) * 0.01  # Forget gate
W_o = np.random.randn(hidden_size, embedding_dim) * 0.01  # Output gate
W_c = np.random.randn(hidden_size, embedding_dim) * 0.01  # Candidate memory

U_i = np.random.randn(hidden_size, hidden_size) * 0.01
U_f = np.random.randn(hidden_size, hidden_size) * 0.01
U_o = np.random.randn(hidden_size, hidden_size) * 0.01
U_c = np.random.randn(hidden_size, hidden_size) * 0.01

b_i = np.zeros((hidden_size, 1))
b_f = np.zeros((hidden_size, 1))
b_o = np.zeros((hidden_size, 1))
b_c = np.zeros((hidden_size, 1))

inputs = np.random.randint(0, 26, size=50)
idx = inputs[0]
x_t = embedding[idx].reshape(-1, 1)

# Output layer
W_y = np.random.randn(output_size, hidden_size) * 0.01
b_y = np.zeros((output_size, 1))



h_t = np.zeros((hidden_size, 1))
c_t = np.zeros((hidden_size, 1))

# Compute LSTM gate activations
f_t = sigmoid(np.dot(W_f, x_t) + np.dot(U_f, h_t) + b_f)
i_t = sigmoid(np.dot(W_i, x_t) + np.dot(U_i, h_t) + b_i)
c_tilde = np.tanh(np.dot(W_c, x_t) + np.dot(U_c, h_t) + b_c)

o_t = sigmoid(np.dot(W_o, x_t) + np.dot(U_o, h_t) + b_o)

# Update cell state and hidden state
c_t = f_t * c_t + i_t * c_tilde
h_t = o_t * np.tanh(c_t)

# Compute output logits
y_t = np.dot(W_y, h_t) + b_y
# outputs.append(y_t)

